{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: PyMC vs. NumPyro on VBJAX\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "    self-contained: true\n",
    "toc: true\n",
    "toc-expand: true\n",
    "execute:\n",
    "  cache: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pure JAX BNM simulation tool [vjax](https://github.com/ins-amu/vbjax/tree/main/vbjax) is used to compare the workflow of PyMC and NumPyro on the same model. The model is a simple MPR based networked simulation.\n",
    "Everything was run with Python 3.11 and the latest versions of the packages.\n",
    "\n",
    "# A simple MPR based simulation in vbjax\n",
    "\n",
    "The model is basically the Readme example of all to all coupled MPR nodes. The simulation is done with the default parameters. The SDE version is used to generate data while the ODE version is used later in statistical model fitting. This procedure is inspired by [this](https://www.pymc.io/projects/examples/en/latest/time_series/Euler-Maruyama_and_SDEs.html) PyMC example, which happens to be provided by the Marseilles theoretical neuroscience group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vbjax as vb\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a network of all to all coupled MPR nodes; Usage of SCs is not easily supported yet as it seems.\n",
    "def network(x, p):\n",
    "    c = 0.03*x.sum(axis=1)\n",
    "    return vb.mpr_dfun(x, c, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5 # number of regions\n",
    "len = 500 # number of samples\n",
    "_, loop_sde = vb.make_sde(dt=0.01, dfun=network, gfun=0.1) # loop_sde is jit compiled via jax\n",
    "_, loop_ode = vb.make_ode(dt=0.01, dfun=network) \n",
    "zs = vb.randn(len, 2, N) # noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run simulation with default parameters\n",
    "xs = loop_sde(zs[0], zs[1:], vb.mpr_default_theta) # loop_sde(ics, noise, parameters)\n",
    "xo = loop_ode(zs[0], np.linspace(0,len, num = len -1), vb.mpr_default_theta) # loop_ode(ics, time, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb.plot_states(xs, 'rV', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb.plot_states(xo, 'rV', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple roundtrip estimation\n",
    "\n",
    "Recover the default parameters and estimate noise from the initial SDE simulation using HMC in NumPyro and PyMC. This is more a proof of concept/ getting familiar with the frameworks than an especially interesting problem. \n",
    "\n",
    "## Disentangle relations\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "    NumPyro --> JAX\n",
    "    \n",
    "```\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "    PyMc --> PyTensor <--> JAX & Numba & C\n",
    "       \n",
    "```\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "    JAX --> XLA --> CPU & GPU & TPU    \n",
    "```\n",
    "\n",
    "tldr: Models/Functions in JAX can be wrapped and used with PyMC. PyTensor gradient graphs can be converted to JAX and vice versa. As NumPyro is pure JAX it can be used from PyMC directly eg as sampler. Staying with PyTensor offers increased  stability, easier debugging (selfproclaimed, not the experience I made) and  mutable graphs but more boilerplate and mental overhead.\n",
    "\n",
    "## NumPyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS, Predictive\n",
    "from jax.random import PRNGKey\n",
    "import jax.random as rng\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(N, len, data=None):\n",
    "    # we could sample the ics as well, but for simplicity we don't\n",
    "    ics = zs[0]\n",
    "\n",
    "    # parameters: tau, I, Delta, J, eta, cr, cv for mpr\n",
    "    theta = numpyro.sample(\n",
    "        \"theta\",\n",
    "        dist.Normal(\n",
    "            loc=jnp.zeros(7),\n",
    "            scale=jnp.ones(7) * 15,\n",
    "        ),\n",
    "    )\n",
    "    curr_theta =  vb.MPRTheta(\n",
    "        tau = theta[0],\n",
    "        I = theta[1],\n",
    "        Delta = theta[2],\n",
    "        J = theta[3],\n",
    "        eta = theta[4],\n",
    "        cr = theta[5],\n",
    "        cv = theta[6]\n",
    "    )\n",
    "    # Predict using the ode model\n",
    "    x = loop_ode(ics, np.linspace(0, len, num = len-1), curr_theta) # loop_ode(ics, time, parameters)\n",
    "\n",
    "    # measurement errors - sample the standard deviation per node\n",
    "    # sigma = numpyro.sample(\"sigma\", dist.LogNormal(0, 1).expand([N]))\n",
    "    sigma = numpyro.sample(\"sigma\", dist.LogNormal(0, 1))#.expand([N]))\n",
    "    # measured populations\n",
    "    numpyro.sample(\"y\", dist.Normal(x, sigma), obs=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: true\n",
    "# use dense_mass for better mixing rate\n",
    "mcmc = MCMC(\n",
    "    NUTS(model, dense_mass=True),\n",
    "    num_warmup=200,\n",
    "    num_samples=500,\n",
    "    num_chains=1,\n",
    "    progress_bar=False #if \"NUMPYRO_SPHINXBUILD\" in os.environ else True,\n",
    ")\n",
    "mcmc.run(PRNGKey(1), N=5, len = 500, data=xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb.mpr_default_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the posterior samples\n",
    "samples = mcmc.get_samples()\n",
    "\n",
    "# Extract the mean value\n",
    "theta_mean = jnp.mean(samples['theta'], axis = 0)\n",
    "\n",
    "# Change a parameter\n",
    "\n",
    "mpr_changed = vb.MPRTheta(\n",
    "        tau = theta_mean[0],\n",
    "        I = theta_mean[1],\n",
    "        Delta = theta_mean[2],\n",
    "        J = theta_mean[3],\n",
    "        eta = theta_mean[4],\n",
    "        cr = theta_mean[5],\n",
    "        cv = theta_mean[6]\n",
    "    )\n",
    "xs_changed = loop_sde(zs[0], zs[1:], mpr_changed)\n",
    "# Plots xs and xs_changed in the same plot with different color\n",
    "names = 'rV'\n",
    "for i in range(xs.shape[1]):\n",
    "        plt.subplot(xs.shape[1], 1, i+1)\n",
    "        plt.plot(xs[:, i], 'k', alpha=0.3)\n",
    "        plt.plot(xs_changed[:, i], 'r', alpha=0.3)\n",
    "        plt.ylabel(names[i])\n",
    "        plt.xlabel('time')\n",
    "        plt.grid(1)\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "from pytensor.graph import Apply, Op\n",
    "from pytensor.link.jax.dispatch import jax_funcify\n",
    "\n",
    "import jax\n",
    "\n",
    "import pymc as pm\n",
    "import pymc.sampling.jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be used to enable float64 in JAX if set True but is has to happen earlier\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead we set the floatX type of pytensor to float32 to make it work with the JAX default \n",
    "dtype = 'float32'   \n",
    "pytensor.config.floatX = dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping a JAX function in a black box style function for PyMC, which requires to define the vector jacobian product (vjp) manually. VBJAX uses named tuples to store parameters which is convenient but has to be wrapped to match the PyMC arraylike interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_fun(params):\n",
    "    curr_theta =  vb.MPRTheta(\n",
    "        tau = params[0],\n",
    "        I = params[1],\n",
    "        Delta = params[2],\n",
    "        J = params[3],\n",
    "        eta = params[4],\n",
    "        cr = params[5],\n",
    "        cv = params[6]\n",
    "    )\n",
    "    # xs = loop_sde(zs[0], zs[1:], curr_theta) # loop_sde(ics, noise, parameters)\n",
    "    xo = loop_ode(zs[0], jnp.linspace(0, 500, num = 500-1), curr_theta) # loop_ode(ics, noise, parameters)\n",
    "\n",
    "    return xo\n",
    "\n",
    "# JAX functions can be compiled multiple times which makes for a convenient incremental building of complex functions\n",
    "jitted_jax_fun = jax.jit(jax_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_jax_fun(np.array(vb.mpr_default_theta, dtype = dtype),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vjp_jax_fun(params, gz):\n",
    "   _, vjp_fn = jax.vjp(jax_fun, params)\n",
    "   return vjp_fn(gz)[0] \n",
    "\n",
    "jitted_vjp_jax_fun = jax.jit(vjp_jax_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_vjp_jax_fun(jnp.array(vb.mpr_default_theta, dtype = dtype), xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that both functions need to be wrapped in a PyTensor compatible OP class that implements:\n",
    "\n",
    "* `make_node`: Creates an Apply node that holds together the symbolic inputs and outputs of our operation\n",
    "* `perform`: Python code that returns the evaluation of our operation, given concrete input values\n",
    "* `grad`: Returns a PyTensor symbolic graph that represents the gradient expression of an output cost wrt to its inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SolOp(Op):\n",
    "    def make_node(self, params):\n",
    "        inputs = [pt.as_tensor_variable(params, dtype = dtype)]\n",
    "        outputs = [pt.tensor3(dtype=dtype)]\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        (params,) = inputs\n",
    "        result = jitted_jax_fun(params)\n",
    "        outputs[0][0] = np.asarray(result, dtype=dtype)\n",
    "\n",
    "    def grad(self, inputs, output_gradients):\n",
    "        (params,) = inputs\n",
    "        (gz,) = output_gradients\n",
    "        return [vjp_sol_op(params, gz)]\n",
    "    \n",
    "class VJPSolOp(Op):\n",
    "    def make_node(self, params, gz):\n",
    "        inputs = [pt.as_tensor_variable(params), pt.as_tensor_variable(gz)]\n",
    "        outputs = [inputs[0].type()]\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        (params, gz) = inputs\n",
    "        result = jitted_vjp_jax_fun(params, gz)\n",
    "        outputs[0][0] = np.asarray(result, dtype=dtype)\n",
    "\n",
    "sol_op = SolOp()\n",
    "vjp_sol_op = VJPSolOp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: false\n",
    "# verify grads, this function helps a lot while debugging as errors in the Op definition easily cause a segfault\n",
    "# pytensor.gradient.verify_grad(sol_op, (jnp.array(vb.mpr_default_theta, dtype = dtype),), rng=np.random.default_rng())\n",
    "pytensor.gradient.verify_grad(sol_op, (jnp.array(vb.mpr_default_theta, dtype = dtype),), mode='DebugMode', rng=np.random.default_rng())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the function to make it available to the PyTensor Linker \n",
    "@jax_funcify.register(SolOp)\n",
    "def sol_op_funcify(op, **kwargs):\n",
    "    return sol_op\n",
    "\n",
    "@jax_funcify.register(VJPSolOp)\n",
    "def vjp_sol_op_funcify(op, **kwargs):\n",
    "    return vjp_sol_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model_pymc:\n",
    "    params = pm.Normal(\"params\", 0, 15, shape = 7)\n",
    "    xo = sol_op(params)\n",
    "    noise = pm.HalfNormal(\"noise\")#, shape = 5)\n",
    "    llike = pm.Normal(\"llike\", mu=xo, sigma=noise, observed=xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model_pymc:    \n",
    "    trace = pm.sample(500, tune=200, chains = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample with numpyro - not working yet. It compiles but then gets NaNs from somewhere\n",
    "# with model_pymc:\n",
    "#     samples = pm.sampling.jax.sample_numpyro_nuts(2000, tune=500, chains = 2, progressbar = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "az.plot_trace(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace)\n",
    "# Plot true values into the forest plot in a hacky way\n",
    "plt.plot(vb.mpr_default_theta, 2.525* np.linspace(1,7, num = 7), \"x\", color=\"r\", alpha=0.4)\n",
    "vb.mpr_default_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_post = np.mean(trace.posterior[\"params\"][0,:,:], axis=0)#[:, 0, :]\n",
    "# Plot theta with variable names on x axis\n",
    "plt.plot(theta_post, \"o\", color=\"k\", ms=10)\n",
    "plt.plot(vb.mpr_default_theta, \"x\", color=\"r\", label=\"True values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_mean = np.asanyarray(theta_post, dtype = dtype)\n",
    "\n",
    "# Change a parameter\n",
    "\n",
    "mpr_changed = vb.MPRTheta(\n",
    "        tau = theta_mean[0],\n",
    "        I = theta_mean[1],\n",
    "        Delta = theta_mean[2],\n",
    "        J = theta_mean[3],\n",
    "        eta = theta_mean[4],\n",
    "        cr = theta_mean[5],\n",
    "        cv = theta_mean[6]\n",
    "    )\n",
    "xs_changed = loop_sde(zs[0], zs[1:], mpr_changed)\n",
    "\n",
    "# Plots xs and xs_changed in the same plot with different color\n",
    "names = 'rV'\n",
    "for i in range(xs.shape[1]):\n",
    "        plt.subplot(xs.shape[1], 1, i+1)\n",
    "        plt.plot(xs[:, i], 'k', alpha=0.3)\n",
    "        plt.plot(xs_changed[:, i], 'r', alpha=0.3)\n",
    "        plt.ylabel(names[i])\n",
    "        plt.xlabel('time')\n",
    "        plt.grid(1)\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal Fazit\n",
    "\n",
    "* Dev Time: The NumPyro Example took less than 1/10th of the time. The reason is probably the two language problem created by PyTensor + JAX  which creates several points of failure in interoperability that are hard to debug. Also looking up PyMC tends to be hard as the API from v3 to v5 seems to have changed a lot resulting in outdated examples etc. Probably diminishes with more experience though the needed boilerplate overhead remains.   \n",
    "\n",
    "* Performance: The NumPyro example is faster but out of the box. This can probably adjusted with experience. Also scaling to more complex models is unknown. Sensible benchmarks are actually hard.\n",
    "\n",
    "* Documentation: Both are good enough.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
